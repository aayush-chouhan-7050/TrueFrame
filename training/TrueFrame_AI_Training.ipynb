{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "I79kgtrVftbA",
        "outputId": "9a4ddb95-31c7-4f97-dd00-7f625c6760df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d441aad-a1e9-4ce6-b063-51cf611f8bf6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d441aad-a1e9-4ce6-b063-51cf611f8bf6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"aayushchouhan7050\",\"key\":\"a95327f6e77e8bc01e09d0681ba3cdd9\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7oGDCeUqgKGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Download Dataset 1: FaceForensics++ (a smaller subset) ---\n",
        "import kagglehub\n",
        "print(\"Downloading FaceForensics++ subset... This may take 10-15 minutes.\")\n",
        "path = kagglehub.dataset_download(\"xdxd003/ff-c23\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# --- Download Dataset 2: Celeb-DF (v2) ---\n",
        "print(\"Downloading Celeb-DF (v2)... This may take 5-10 minutes.\")\n",
        "path = kagglehub.dataset_download(\"reubensuju/celeb-df-v2\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "print(\"\\nAll datasets downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKxwpscFgPJg",
        "outputId": "5aac0a6a-e2be-46b5-fdcf-c97db114cd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading FaceForensics++ subset... This may take 10-15 minutes.\n",
            "Using Colab cache for faster access to the 'ff-c23' dataset.\n",
            "Path to dataset files: /kaggle/input/ff-c23\n",
            "Downloading Celeb-DF (v2)... This may take 5-10 minutes.\n",
            "Using Colab cache for faster access to the 'celeb-df-v2' dataset.\n",
            "Path to dataset files: /kaggle/input/celeb-df-v2\n",
            "\n",
            "All datasets downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Creating the ultimate combined dataset...\")\n",
        "\n",
        "# --- Define Paths based on your download locations ---\n",
        "# Source paths for the unzipped datasets\n",
        "FFPP_REAL_PATH = '/kaggle/input/ff-c23/FaceForensics++_C23/original'\n",
        "FFPP_FAKE_PATH = '/kaggle/input/ff-c23/FaceForensics++_C23/Deepfakes'\n",
        "CELEBDF_REAL_PATH = '/kaggle/input/celeb-df-v2/Celeb-real'\n",
        "CELEBDF_FAKE_PATH = '/kaggle/input/celeb-df-v2/Celeb-synthesis'\n",
        "\n",
        "# Destination paths for our new combined dataset\n",
        "ULTIMATE_REAL_PATH = 'ultimate_dataset/train/real/'\n",
        "ULTIMATE_FAKE_PATH = 'ultimate_dataset/train/fake/'\n",
        "\n",
        "# --- Create Destination Folders ---\n",
        "os.makedirs(ULTIMATE_REAL_PATH, exist_ok=True)\n",
        "os.makedirs(ULTIMATE_FAKE_PATH, exist_ok=True)\n",
        "\n",
        "# --- List of sources and destinations to process ---\n",
        "tasks = [\n",
        "    (FFPP_REAL_PATH, ULTIMATE_REAL_PATH),\n",
        "    (FFPP_FAKE_PATH, ULTIMATE_FAKE_PATH),\n",
        "    (CELEBDF_REAL_PATH, ULTIMATE_REAL_PATH),\n",
        "    (CELEBDF_FAKE_PATH, ULTIMATE_FAKE_PATH)\n",
        "]\n",
        "\n",
        "# --- Copying Logic ---\n",
        "def copy_videos(source_dir, dest_dir):\n",
        "    # Find all .mp4 files in the source directory\n",
        "    video_files = glob.glob(os.path.join(source_dir, '*.mp4'))\n",
        "\n",
        "    # Copy each file to the destination\n",
        "    for video_path in tqdm(video_files, desc=f\"Copying from {os.path.basename(os.path.normpath(source_dir))}\"):\n",
        "        shutil.copy(video_path, dest_dir)\n",
        "\n",
        "# --- Execute the Copying ---\n",
        "for source, dest in tasks:\n",
        "    if os.path.exists(source):\n",
        "        copy_videos(source, dest)\n",
        "    else:\n",
        "        print(f\"Warning: Source directory not found, skipping: {source}\")\n",
        "\n",
        "print(\"\\n✅ All videos have been combined into 'ultimate_dataset'!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug3cEzsigS8f",
        "outputId": "a6e617ee-9000-490e-ad2a-a6dc08de8693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the ultimate combined dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying from original: 100%|██████████| 1000/1000 [00:22<00:00, 43.74it/s]\n",
            "Copying from Deepfakes: 100%|██████████| 1000/1000 [00:22<00:00, 44.25it/s]\n",
            "Copying from Celeb-real: 100%|██████████| 590/590 [00:12<00:00, 45.91it/s]\n",
            "Copying from Celeb-synthesis: 100%|██████████| 5639/5639 [01:55<00:00, 48.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ All videos have been combined into 'ultimate_dataset'!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import shutil\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "# --- Configure Logging ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "class FrameExtractor:\n",
        "    def __init__(self, real_videos_path: Path, fake_videos_path: Path, output_dir: Path,\n",
        "                 frame_interval: int = 30, max_workers: int = None,\n",
        "                 max_videos_per_class: int = None, max_frames_per_video: int = None): # <-- New options\n",
        "        self.real_videos_path = real_videos_path\n",
        "        self.fake_videos_path = fake_videos_path\n",
        "        self.output_dir = output_dir\n",
        "        self.frame_interval = frame_interval\n",
        "        self.max_workers = max_workers if max_workers is not None else os.cpu_count()\n",
        "        self.max_videos_per_class = max_videos_per_class # <-- New\n",
        "        self.max_frames_per_video = max_frames_per_video # <-- New\n",
        "\n",
        "        self.real_frames_dir = self.output_dir / 'train/real'\n",
        "        self.fake_frames_dir = self.output_dir / 'train/fake'\n",
        "\n",
        "    def _setup_directories(self) -> None:\n",
        "        logging.info(\"Setting up output directories...\")\n",
        "        if self.output_dir.exists():\n",
        "            shutil.rmtree(self.output_dir)\n",
        "        self.real_frames_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.fake_frames_dir.mkdir(parents=True, exist_ok=True)\n",
        "        logging.info(\"Output directories are ready.\")\n",
        "\n",
        "    def _process_single_video(self, video_path: Path) -> str:\n",
        "        try:\n",
        "            output_dir = self.real_frames_dir if video_path.parent.name == 'real' else self.fake_frames_dir\n",
        "            cap = cv2.VideoCapture(str(video_path))\n",
        "            if not cap.isOpened():\n",
        "                raise IOError(f\"Cannot open video file: {video_path.name}\")\n",
        "\n",
        "            frame_count = 0\n",
        "            extracted_count = 0 # <-- New: track extracted frames\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # New: Stop if we've extracted enough frames for this video\n",
        "                if self.max_frames_per_video and extracted_count >= self.max_frames_per_video:\n",
        "                    break\n",
        "\n",
        "                if frame_count % self.frame_interval == 0:\n",
        "                    frame_path = output_dir / f\"{video_path.stem}_frame_{frame_count}.jpg\"\n",
        "                    cv2.imwrite(str(frame_path), frame)\n",
        "                    extracted_count += 1\n",
        "\n",
        "                frame_count += 1\n",
        "            cap.release()\n",
        "            return video_path.name\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process {video_path.name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def run(self) -> None:\n",
        "        self._setup_directories()\n",
        "\n",
        "        real_videos = list(self.real_videos_path.glob('*.mp4'))\n",
        "        fake_videos = list(self.fake_videos_path.glob('*.mp4'))\n",
        "\n",
        "        # --- New: Shuffle and limit the number of videos ---\n",
        "        random.shuffle(real_videos)\n",
        "        random.shuffle(fake_videos)\n",
        "\n",
        "        if self.max_videos_per_class:\n",
        "            real_videos = real_videos[:self.max_videos_per_class]\n",
        "            fake_videos = fake_videos[:self.max_videos_per_class]\n",
        "\n",
        "        all_videos = real_videos + fake_videos\n",
        "\n",
        "        if not all_videos:\n",
        "            logging.warning(\"No videos found in the source directories.\")\n",
        "            return\n",
        "\n",
        "        logging.info(f\"Processing a random subset of {len(all_videos)} total videos.\")\n",
        "        logging.info(f\"Starting parallel extraction with {self.max_workers} workers.\")\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            futures = [executor.submit(self._process_single_video, video) for video in all_videos]\n",
        "            for future in tqdm(as_completed(futures), total=len(all_videos), desc=\"Extracting frames\"):\n",
        "                future.result()\n",
        "\n",
        "        logging.info(\"✅ Frame extraction complete!\")\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Define your configuration with the new limits\n",
        "    config = {\n",
        "        \"real_videos_path\": Path('ultimate_dataset/train/real/'),\n",
        "        \"fake_videos_path\": Path('ultimate_dataset/train/fake/'),\n",
        "        \"output_dir\": Path('frames_ultimate_subset'), # Use a new output folder name\n",
        "        \"frame_interval\": 30,\n",
        "        \"max_workers\": os.cpu_count(),\n",
        "        # --- NEW LIMITS ---\n",
        "        \"max_videos_per_class\": 1500,  # Process 1500 real and 1500 fake videos\n",
        "        \"max_frames_per_video\": 15      # Extract a max of 15 frames from each video\n",
        "    }\n",
        "\n",
        "    # 2. Create an instance and run\n",
        "    extractor = FrameExtractor(**config)\n",
        "    extractor.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN4a8sr6hN6r",
        "outputId": "20e71b14-089f-4a37-ff8f-ef13b4f8a498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting frames: 100%|██████████| 3000/3000 [1:04:21<00:00,  1.29s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Starting FINAL model fine-tuning on the ultimate dataset...\")\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Paths for the final run ---\n",
        "DATA_DIR = 'frames_ultimate_subset/train' #<-- Path to your new subset of frames\n",
        "PREVIOUS_MODEL_PATH = 'deepfake_detector_best_model.pth' #<-- The model we are improving\n",
        "FINAL_MODEL_SAVE_PATH = 'deepfake_detector_ultimate_model.pth' #<-- The final output!\n",
        "\n",
        "# --- Fine-Tuning Parameters ---\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0001 #<-- Use a smaller learning rate for fine-tuning\n",
        "\n",
        "# --- 2. Aggressive Data Augmentation ---\n",
        "# Transforms to simulate real-world conditions like bad lighting and compression\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 3. Data Preparation ---\n",
        "full_dataset = datasets.ImageFolder(DATA_DIR)\n",
        "class_names = full_dataset.classes\n",
        "\n",
        "train_size = int(0.9 * len(full_dataset)) # Use 90% for training\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_indices, val_indices = random_split(range(len(full_dataset)), [train_size, val_size])\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(full_dataset, train_indices.indices)\n",
        "train_dataset.dataset.transform = train_transforms\n",
        "\n",
        "val_dataset = torch.utils.data.Subset(full_dataset, val_indices.indices)\n",
        "val_dataset.dataset.transform = val_transforms\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "print(f\"Data prepared: {len(train_dataset)} training images, {len(val_dataset)} validation images.\")\n",
        "\n",
        "# --- 4. Class Weighting for the new dataset ---\n",
        "train_labels = [full_dataset.targets[i] for i in train_indices.indices]\n",
        "class_counts = Counter(train_labels)\n",
        "num_samples = len(train_labels)\n",
        "weights = [num_samples / (len(class_names) * class_counts[i]) for i in range(len(class_names))]\n",
        "class_weights_tensor = torch.FloatTensor(weights).to(device)\n",
        "print(f\"New class weights: {class_weights_tensor}\")\n",
        "\n",
        "# --- 5. Load Your Best Model for Fine-Tuning ---\n",
        "model = models.efficientnet_b0()\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, len(class_names))\n",
        "\n",
        "# Load the weights from your previous best model\n",
        "model.load_state_dict(torch.load(PREVIOUS_MODEL_PATH))\n",
        "model = model.to(device)\n",
        "print(\"Previous best model loaded successfully for fine-tuning.\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- 6. The Training and Validation Loop ---\n",
        "print(\"\\nStarting final training run...\")\n",
        "start_time = time.time()\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # -- TRAINING --\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_dataloader, desc=\"Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Training Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # -- VALIDATION --\n",
        "    model.eval()\n",
        "    running_corrects = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_dataloader, desc=\"Validating\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "    epoch_acc = running_corrects.double() / len(val_dataset)\n",
        "    print(f'Validation Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "    if epoch_acc > best_val_accuracy:\n",
        "        best_val_accuracy = epoch_acc\n",
        "        torch.save(model.state_dict(), FINAL_MODEL_SAVE_PATH)\n",
        "        print(f\"New best model saved with accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "# --- 7. Conclusion ---\n",
        "time_elapsed = time.time() - start_time\n",
        "print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "print(f'Best Validation Accuracy: {best_val_accuracy:.4f}')\n",
        "print(f\"✅ Your ultimate model is saved at: {FINAL_MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QSYbHf00Lgx",
        "outputId": "177a4418-7bca-4835-cf74-f3d86c3f6859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FINAL model fine-tuning on the ultimate dataset...\n",
            "Using device: cuda:0\n",
            "Data prepared: 35271 training images, 3919 validation images.\n",
            "New class weights: tensor([1.0261, 0.9752], device='cuda:0')\n",
            "Previous best model loaded successfully for fine-tuning.\n",
            "\n",
            "Starting final training run...\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [05:14<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:28<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8650\n",
            "New best model saved with accuracy: 0.8650\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:55<00:00,  3.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:26<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9155\n",
            "New best model saved with accuracy: 0.9155\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:45<00:00,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:27<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9428\n",
            "New best model saved with accuracy: 0.9428\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:48<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:26<00:00,  4.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9485\n",
            "New best model saved with accuracy: 0.9485\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:44<00:00,  3.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:26<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9513\n",
            "New best model saved with accuracy: 0.9513\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:48<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:27<00:00,  4.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9474\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:55<00:00,  3.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:26<00:00,  4.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9633\n",
            "New best model saved with accuracy: 0.9633\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:54<00:00,  3.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:27<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9645\n",
            "New best model saved with accuracy: 0.9645\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:44<00:00,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:27<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9696\n",
            "New best model saved with accuracy: 0.9696\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1103/1103 [04:49<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 123/123 [00:28<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9732\n",
            "New best model saved with accuracy: 0.9732\n",
            "\n",
            "Training complete in 53m 17s\n",
            "Best Validation Accuracy: 0.9732\n",
            "✅ Your ultimate model is saved at: deepfake_detector_ultimate_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = 'frames_ultimate_subset/train'\n",
        "# --- IMPORTANT: Update this path ---\n",
        "MODEL_PATH = 'deepfake_detector_ultimate_model.pth'\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- 2. Prepare Validation Data ---\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "full_dataset = datasets.ImageFolder(DATA_DIR, val_transforms)\n",
        "class_names = full_dataset.classes\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "_, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# --- 3. Load the Trained Model ---\n",
        "model = models.efficientnet_b0()\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, len(class_names))\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# --- 4. Get Predictions ---\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(val_dataloader, desc=\"Evaluating final model\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# --- 5. Display Metrics ---\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Final Model Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "F8BRqGAeBIj6",
        "outputId": "35f1eeab-18bc-436d-c37f-1801e0a41de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating final model: 100%|██████████| 123/123 [00:28<00:00,  4.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.99      1.00      0.99      1905\n",
            "        real       1.00      0.99      0.99      2014\n",
            "\n",
            "    accuracy                           0.99      3919\n",
            "   macro avg       0.99      0.99      0.99      3919\n",
            "weighted avg       0.99      0.99      0.99      3919\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# --- IMPORTANT: Update this path ---\n",
        "MODEL_PATH = 'deepfake_detector_ultimate_model.pth'\n",
        "class_names = ['fake', 'real']\n",
        "\n",
        "# --- 2. Load the Model ---\n",
        "model = models.efficientnet_b0()\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, len(class_names))\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# --- 3. Prediction Function (using the same one as before) ---\n",
        "inference_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def predict_video(video_path, frame_interval=30):\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}\")\n",
        "        return\n",
        "    predictions = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        if cap.get(cv2.CAP_PROP_POS_FRAMES) % frame_interval == 0:\n",
        "            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            image_tensor = inference_transforms(pil_image).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(image_tensor)\n",
        "                _, pred_idx = torch.max(outputs, 1)\n",
        "                predictions.append(pred_idx.item())\n",
        "    cap.release()\n",
        "\n",
        "    if not predictions:\n",
        "        print(\"Could not extract any frames.\")\n",
        "        return\n",
        "\n",
        "    fake_count = predictions.count(class_names.index('fake'))\n",
        "    real_count = len(predictions) - fake_count\n",
        "    fake_percentage = (fake_count / len(predictions)) * 100\n",
        "\n",
        "    print(f\"\\n--- Final Prediction for: {os.path.basename(video_path)} ---\")\n",
        "    verdict = \"FAKE\" if fake_count > real_count else \"REAL\"\n",
        "    confidence = fake_percentage if verdict == \"FAKE\" else 100 - fake_percentage\n",
        "\n",
        "    if verdict == \"FAKE\":\n",
        "        print(f'Verdict: ৯০% The video is likely FAKE ({confidence:.2f}% confidence)')\n",
        "    else:\n",
        "        print(f'Verdict: ✅ The video is likely REAL ({confidence:.2f}% confidence)')\n",
        "\n",
        "# --- 4. RUN THE TEST ---\n",
        "# --- IMPORTANT: Update this path to your Instagram Reel video ---\n",
        "video_to_test = \"/content/173239-848944192_small.mp4\"\n",
        "predict_video(video_to_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEc_ow-oBe9p",
        "outputId": "c01f402c-860c-4e2a-b5b9-e070e9316e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Prediction for: 173239-848944192_small.mp4 ---\n",
            "Verdict: ✅ The video is likely REAL (100.00% confidence)\n"
          ]
        }
      ]
    }
  ]
}